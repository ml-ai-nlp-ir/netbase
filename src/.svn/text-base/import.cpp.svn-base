#pragma once
// strcmp
#include <string.h>
// malloc, exit:
#include <cstdlib>
//#include <errno.h>  // just use perror("blah")! Adds errorstr(errno) automagically

#include <sys/sem.h> // sudo ipcrm -M 0x0410669190
#include <regex.h>

#ifdef sqlite3
#include "sqlite3.h"
#endif



#include "netbase.hpp"
#include "util.hpp"

#include "import.hpp"
#include "relations.hpp"// for wordnet

// 64 BIT : %x -> %016llX
string basepath = "./import/";
char* nodes_file = "data/nodes.txt";
char* statements_file = "data/statements.txt";
char* images_file = "data/images.txt";


/* attach to the segment to get a pointer to it: */
	const void * shmat_root = (const void *) 0x101000000; // mac 64 bit
//const void * shmat_root = (const void *)0x105800000;//test

//#define sharedLib

#ifdef sharedLib

void __attribute__((constructor)) my_init(void) {
	if (debug)
		printf("loading shared library \n");
	share_memory();
	if (root_memory[0] != 1)
		buildDictionary(); //4sec.
	else if (debug)
		printf("attached to shared memory of library libDictionary.so \n");
	// saveDictionary();
	// loadDictionary();
	return 0;
}
#endif



//#include <sys/types.h>
//#include <sys/ipc.h>
#include <sys/shm.h>
/*
The shared library's read-only segment (in particular, its .text section) can be shared among all processes; while its write-able sections (such as .data and .bss) can be allocated uniquely for each executing process. This write-able segment is also referred to as the object's Static Data Segment. It is this static data segment that creates most of the complexity for the implementation of shared libraries.
http://www.cadenux.com/xflat/NoMMUSharedLibs.html#shlibs
 */
//#define	IPC_CREAT	0x01000		/* Create entry if key does not exist */
#define READ_WRITE     0666// 0x666

int semrm(key_t key, int id = 0) {
	union semun arg;
	id = semget(key, 0, 0);
	if (id == -1)
		return -1;
	return semctl(id, 0, IPC_RMID, arg);
}

int share_memory() {
	if (root_memory) {
		ps("root_memory already attached!");
		return 0;
	}
	/* make the key: */
	int key = 0x0410669190; //0x57020303;// #netbase ftok("netbase", 'RW');
	int shmid;
	//                virgin_memory=0;

	if ((shmid = shmget(key, sizeOfSharedMemory, READ_WRITE)) == -1) {
		ps("share_memory used for the first time");
		virgin_memory = 1;
		if ((shmid = shmget(key, sizeOfSharedMemory, READ_WRITE | IPC_CREAT)) == -1){
			semrm(key); // clean and try again
		if ((shmid = shmget(key, sizeOfSharedMemory, READ_WRITE | IPC_CREAT)) == -1) {
			perror("share_memory failed: shmget! Not enough memory?");
//			printf("try calling ./clear-shared-memory.sh\n");
			//			perror(strerror(errno)); <= ^^ redundant !!!
			printf("try ipcclean && sudo ipcrm -M 0x0410669190");
			exit(1);
		}
		}
	}
	root_memory = (char *) shmat(shmid, (const void *) shmat_root, 0);
	if (root_memory == 0 || root_memory == (void *) (-1)) {//virgin_memory=1;
		ps("receiving other share_memory address");
		root_memory = (char *) shmat(shmid, (const void *) 0, 0); //void
	}
	if (root_memory == 0 || root_memory == (void *) (-1)) {
		perror("share_memory failed: shmat! Not enough memory?");
		exit(1);
	}
	Context* c = currentContext(); // getContext(node->context);
	if ((char*) root_memory != (char*) shmat_root) {// 64 BIT : %x -> %016llX
		printf("FYI: root_memory != desired shmat_root %016llX!=%016llX \n", root_memory, shmat_root);
		fixPointers();
	}
	char* msg = "share_memory root address = %016llX	size = %x	max = %016llX\n";
	printf(msg, root_memory, sizeOfSharedMemory, (char*) root_memory + sizeOfSharedMemory);
	contexts = (Context*) root_memory;
	abstracts = (Ahash*) (root_memory + abstractsOffset); // reuse or reinit
	extrahash = (Ahash*) (root_memory + abstractsOffset + abstractHashSize);
	checkRootContext();
	return 0;
}

long getMemory() {
	//long phypz = sysconf(_SC_PHYS_PAGES);
	//long psize = sysconf(_SC_PAGE_SIZE);
	//return phypz*psize;
}

long GetAvailableMemory(void) {
	void *p, *q;
	long siz = 10000000;
	q = p = calloc(1, siz);
	while (q) {
		siz = siz * 1.2; // Can be more to speed up things
		q = realloc(p, siz);
		if (q)// infinite virtual mem :{ 31107287834197
			p = q;
	}
	free(p);
	return siz;
}

// modify char* in vivo / inline!

void norm(char* title) {
	int len = strlen(title);
	for (int i = len; i >= 0; --i) {
		if (title[i] == ' ' || title[i] == '_' || title[i] == '-') {
			strcpy(&title[i], &title[i + 1]); //,len-i);
		}
	}
}
map<long, string> nodeNameImages;
map<long, string> nodeNameImages2; // chopped
//map<long,string> nodeNameImages3;// chopped image name

void importImages() {// 18 MILLION!   // 18496249
	p("image import starting ...");
	FILE *infile;
	char line[100];
	int linecount = 0;
	Node* wiki_image = getAbstract("wiki_image");
	addStatement(wiki_image, is_a, getThe("image"));

	/* Open the file.  If NULL is returned there was an error */

	printf("Opening File %s\n", (basepath + images_file).data());
	if ((infile = fopen((basepath + images_file).data(), "r")) == NULL) {
		perror("Error opening file");
		exit(1);
	}
	char tokens[1000];
	char image[1000];
	char title[1000];
	int good = 0;
	int bad = 0;
	while (fgets(line, sizeof (line), infile) != NULL) {
		if (++linecount % 10000 == 0) {
			pi(linecount);
		};
		sscanf(line, "%s %*s %s", title, image);
		if (!hasWord(title))
			norm(title); //blue -_fin ==> bluefin
		if (!hasWord(title)) {// currently only import matching words.
			//            if(++bad%1000==0){ps("bad image (without matching word) #");pi(bad);}
			continue;
		}
		if (getImage(title) != "")
			continue; //already has one ; only one so far!
		Node* subject = getAbstract(title);
		Node* object = getAbstract(image); // getThe(image);;
		addStatement(subject, wiki_image, object, false);
		if (++good % 1000 == 0) {
			ps("GOOD images:");
			pi(good);
		}
	}
	fclose(infile);

	good = 0;
	Node* object = getAbstract(image); // getThe(image);;
	/*
	// again, this time with word fragments
	 * MEMORY LEAK!? where??
	infile = fopen((basepath + images_file).data(), "r");
	  while (fgets(line, sizeof (line), infile) != NULL) {
		if (++linecount % 1000 == 0)pi(linecount);
		sscanf(line, "%s %*s %s", title, image);
//        #blue fin ==> blue and fin
		char* word=title;
		for (int i = 0; i < strlen(title); i++) {
			if(title[i]==' '||title[i]=='_'||title[i]=='-'){//split
				title[i]=0;
//                nodeNameImages2[hash(title)]=image;
				if(!hasWord(word))continue;
				if(getImage(word)!="")continue;// only one so far!
				addStatement(getAbstract(word), wiki_image, object,false);
				if(++good%1000==0){ps("!!good");pi(good);}

				word=&title[i+1];
			}
		}
	}
	 */
	//    for(map<long,string>::const_iterator iter = nodeNameImages2.begin(); iter != nodeNameImages2.end(); ++iter){
	//        long key=iter->first;
	//        string value=iter->second;
	//        if(nodeNameImages.find(key)==nodeNameImages.end())// does not exist?
	//            nodeNameImages[key]=value;
	//    }
	//    for(map<long,Node*>::const_iterator iter = abstracts->begin(); iter != abstracts->end(); ++iter){
	//        long key=iter->first;
	//        Node* node=iter->second;
	//        string image=nodeNameImages[key];
	////        if(!image)key=hash2(node->name)
	//        Node* object = getAbstract(image.data());// getThe(image);;
	//        addStatement(node, predicate, object,false);
	//    }
	fclose(infile);
	p("done image import");
}

void importNodes() {
	p("node import starting ...");
	FILE *infile;
	// char fname[40]="/Users/me/data/base/netbase.sql";
	//  ('2282','Anacreon','N'),

	// char* fname="/Users/me/data/base/netbase/nodes.test";
	//2026896532	103	dry_out	Verb	\N	\N	11	103

	char line[100];
	int linecount = 0;
	/* Open the file.  If NULL is returned there was an error */
	printf("Opening File %s\n", (basepath + nodes_file).data());
	if ((infile = fopen((basepath + nodes_file).data(), "r")) == NULL) {
		perror("Error opening file");
		exit(1);
	}
	while (fgets(line, sizeof (line), infile) != NULL) {
		char tokens[1000];
		/* Get each line from the infile */

		if (++linecount % 1000 == 0)
			pi(linecount);
		strcpy(tokens, line);
		int x = 0; // replace ' ' with '_'
		while (tokens[x]) {
			if (tokens[x] == ' ')
				tokens[x] = '_';
			x++;
		}
		// char name[1000];
		char* name = (char*) malloc(100);
		// char kind[20];
		char contextId_s[100];
		char deleted[1];
		char version[1];
		char wordKindName[100];
		int wordKind;
		int Id;
		int kind;
		int contextId = wordnet;
		int contextID;
		//2026896532	103	dry_out	Verb	\N	\N	11	103
		//	sscanf(tokens,"%d\t%s\t%s\t%s\t%*s\t%*s\t%*d\t%d",&Id,contextId_s,name,wordKindName,&kind);
		sscanf(tokens, "%d\t%s\t%s\t%*s\t%*s\t%*s\t%d\t%*d", &Id, contextId_s, name, &kind); // wordKind->kind !
		if (kind == 105)kind = _relation; //relation
		if (kind == 1)kind = _concept; //noun
		//        if(kind==1)continue;
		if (kind == 103)kind = noun; //noun
		if (kind == 10)kind = noun; //noun
		if (kind == 11)kind = verb; //verb
		if (kind == 12)kind = adjective; //adjective
		if (kind == 13)kind = adverb; //adverb
		//        contextId = atoi(contextId_s);
		//
		//        if (Id < 1000)contextId = wordnet; //wn
		//        if (contextId == 100 && Id > 1000)contextId = 100;
		Node* n;
		if (Id > 1000)
			n = add(name, kind, contextId);
		else
			n = add_force(contextId, Id, name, kind);
		if (n == 0) {
			p("out of memory");
			break;
		}
		wn_map[Id] = n->id;
		wn_map2[n->id] = Id;
	}
	fclose(infile); /* Close the file */
	p("node import ok");
}

void importStatements() {
	FILE *infile;

	char line[1000];
	int linecount = 0;

	/* Open the file.  If NULL is returned there was an error */
	printf("Opening File %s\n", (basepath + statements_file).data());
	if ((infile = fopen((basepath + statements_file).data(), "r")) == NULL) {
		perror("Error opening file");
		exit(1);
	}
	char tokens[1000];
	while (fgets(line, sizeof (line), infile) != NULL) {
		/* Get each line from the infile */
		if (++linecount % 1000 == 0)
			pi(linecount);
		// p(line);
		//	strcpy(  tokens,line);
		//		int x = 0;
		//    while (tokens[x++])
		//        if (tokens[x]==' ')
		//		    tokens[x]='_';
		int contextId;
		int subjectId;
		int predicateId;
		int objectId;
		int id; // ignore now!!
		sscanf(line, "%d\t%d\t%d\t%d", &id, &subjectId, &predicateId, &objectId); // no contextId? cause this db assumes GLOBAL id!
		subjectId = wn_map[subjectId]; //%1000000;//wn!!
		predicateId = predicateId; //%1000000;
		if (predicateId > 100)
			predicateId = wn_map[predicateId]; //%1000000;
		objectId = wn_map[objectId]; //%1000000;
		// printf("%d\t%d\t%d\n",subjectId,predicateId,objectId);
		if (subjectId < 1000 || objectId < 1000 || predicateId == 50 || predicateId == 0 || subjectId == 1043 || subjectId == 1044)continue;
		Statement* s = addStatement4(wordnet, subjectId, predicateId, objectId);
	}
	fclose(infile); /* Close the file */
	p("statements import ok");
}
#ifdef sqlite3
int maxBytes = 1000000;

void importSqlite(char* filename) {
	sqlite3* db;
	sqlite3_stmt *statement;
	const char* unused = (char*) malloc(1000);
	int status = sqlite3_open(filename, &db);
	pi(status);
	status = sqlite3_prepare(db, "select * from nodes;", maxBytes, &statement, &unused);
	pi(status);

	// http://www.sqlite.org/c3ref/step.html
	status = sqlite3_step(statement);
	pi(status);
	if (status == SQLITE_DONE) {
		p("No results");
	}
	if (status == SQLITE_ROW) {
		// http://www.sqlite.org/c3ref/column_blob.html
		int id = sqlite3_column_int(statement, 0);
		const unsigned char *text = sqlite3_column_text(statement, 1);
		printf("text %s\n", text);
	}
	sqlite3_finalize(statement);
	sqlite3_close(db);
}
#endif

string deCamel(string s) {
	static string space = " ";
	for (int i = s.length(); i > 1; i--) {
		char c = s[i];
		if (c > 65 && c < 91)s.replace(i, 1, space + (char) tolower(c));
		if (c == '(')s[i - 1] = 0; //cut _( )
		if (c == ',')s[i] = 0; //cut ,_
		if (c == ')')s[i] = 0;
	}
	//    s[0]=tolower(s[0]);
	s = replace_all(s, "_", " ");
	s = replace_all(s, "  ", " ");
	return s;
};

// TODO:!test MEMLEAK!!

const char* parseWikiTitle(char* item, int id = 0, int context = current_context) {
	//    string s="wordnet_raw_material_114596700";
	static string s;
	s = string(item); //"Uncanny_Tales_(Canadian_pulp_magazine)";
	//    South_Taft,_California
	// facts/hasPopulationDensity/ArticleExtractor.txt:205483101	Sapulpa,_Oklahoma	397.1#/km^2	0.9561877303748038


	//    string s="wordnet_yagoActorGeo_1";
	//    wordnet_yagoActorGeo_1
	//    if(s.find("wordnet"))contextId=wordnet;
	//if(s.find("wikicategory")) kind=list / category
	s = replace_all(s, "wikicategory_", "");
	s = replace_all(s, "wordnet_", "");
	s = replace_all(s, "yago_", "");
	s = replace_all(s, "wikicategory", "");
	s = replace_all(s, "wordnet", "");
	s = replace_all(s, "yago", "");
	int last = s.rfind("_");
	int type = s.find("(");
	string clazz = deCamel(s.substr(type + 1, -1));
	string word = s;
	string Id = s.substr(last + 1, -1);
	//    int
	id = atoi(Id.c_str());
	if (id > 0) {
		word = s.substr(0, last);
	}
	//    word=deCamel(word);
	//    item=word.c_str();
	//    ~word;
	return word.c_str(); // TODO!! MEMLEAK!!
}

void extractUnit() {
	string s = "397.1#/km^2";
}

int getFields(char* line, vector<char*>& fields, const char* separator = ",;\t", int nameRowNr = -1, const char* nameRow = 0) {
	char * token;
	token = strtok(line, separator);
	int row = 0;
	while (token != NULL) {
		fields.push_back(token);
		if (nameRowNr < 0) {
			if (nameRow == 0) {
				if (eq("name", token))nameRowNr = row;
				if (eq("Name", token))nameRowNr = row;
				if (eq("title", token))nameRowNr = row;
				if (eq("Title", token))nameRowNr = row;
			} else if (eq(nameRow, token))nameRowNr = row;
		}
		//    printf ("%s\n",pch);
		token = strtok(NULL, separator);
		row++;
	}
	return nameRowNr;
}

void fixNewline(char* line) {
	int len = strlen(line);
	if (line[len - 1] == '\n')
		line[--len] = 0;
	if (line[len - 1] == '\r')
		line[--len] = 0;
}

char* extractTagName(char* line) {
	return match(line, "<([^>]+)>");
}

char* extractTagValue(char* line) {
	return match(line, ">([^<]+)<");
}

bool isNameField(char* field, char* nameField) {
	if (nameField && !eq(field, nameField))return false;
	if (eq(field, nameField))return true;
	if (eq(field, "name"))return true;
	if (eq(field, "title"))return true;
	if (eq(field, "label"))return true;
	return false;
}

Node* namify(Node* node, char* name) {
	node->name = name; // RENAME NODE !!! ok???
	addStatement(getAbstract(name), Instance, node);
	// replaceNode(subject,object);
	return node;
}

void addAttributes(Node* subject, char* line) {
	line = (char*) replace_all(line, "\"", "'").c_str();
	do {
		char* attribute = match(line, " ([^=]+)='[^']'");
		char* value = match(line, " [^=]+='([^'])'");
		if (!attribute)break;
		Node* predicate = getThe(attribute);
		Node* object = getThe(value);
		addStatement(subject, predicate, object);
	} while (attribute != 0);
}

bool hasAttribute(char* line) {
	return match(line, " ([^=]+)=");
}

//Context* context,
//Node* type,
// importXml("/Users/me/data/base/geo/geolocations/Orte_und_GeopIps_mit_PLZ.xml","city","ort");
void importXml(const char* facts_file, char* nameField, const char* ignoredFields, const char* includedFields) {
	p("import csv start");
	bool dissect = false;
	char line[1000];
	char* line0= (char*) malloc(sizeof (char*) *100);
	char* field = (char*) malloc(sizeof (char*) *100);
	char* value = (char*) malloc(sizeof (char*) *10000000);

	Node* root=0;
	Node* parent=0;// keep track of 1 layer
	Node* subject=0;
	Node* predicate=0;
	Node* object=0;
	vector<Node*> predicates = *new vector<Node*>();
	map<char*, char*> fields;
	queue<Node*> parents; //constructed!!

//	char* objectName = (char*) malloc(100);
//	int depth = 0;
	//	vector<char*> ignoreFields = splitString(ignoredFields, ",");
	//	vector<char*>& includeFields = splitString(includedFields, ",");
	int linecount = 0;
	FILE *infile;
	printf("Opening XML File %s\n", (facts_file));
	if ((infile = fopen((facts_file), "r")) == NULL)facts_file = (basepath + facts_file).c_str();
	if ((infile = fopen((facts_file), "r")) == NULL) {
		perror("Error opening file");
		exit(1);
	}
	//	map<Node*,Node*> fields;
	while (fgets(line, sizeof (line), infile) != NULL) {
		if (!line)break;
		if (++linecount % 1000 == 0)printf("%d\n", linecount);
		fixNewline(line);
		line0=line;// readable in Debugger!
		if (contains(line, "<?xml"))continue;
		if (!subject) {
			if(root)
				subject = add(extractTagName(line));
			else
				root= add(extractTagName(line));
			continue;
		}

		if (startsWith(line, "</")) {
			//			for(Node* predicate:fields){
			//				object = getThe(extractTagValue(line),null, dissect);
			//				Statement *s = addStatement(subject, predicate, object, false);
			//				showStatement(s);
			//			}
//			if(!parents.empty())
			subject = 0;// parents.back(); // back????
//			parents.pop();
			fields.clear();
			continue;
		}
		// <address> <city> <zip>12345</zip> <name>bukarest</name> </city> </address>
		if (match(line, "<[a-z]") && contains(line, "</")) {
			field = extractTagName(line);
			value = extractTagValue(line);
			if(ignoredFields&&contains(ignoredFields,field))
				continue;
			if (isNameField(field, nameField)) {
//				parent=parents.front();
				// rewrite address.member=city
//				deleteStatement(findStatement(parent, Member, subject, 0, 0, 0, 0));
				// address.CITY=bukarest
				predicate = getThe(subject->name); // CITY
				Node* object = namify(subject, value); // city.name=bukarest
				addStatement(parent, predicate, object,DONT_CHECK_DUPLICATES); // address.CITY=bukarest
				subject = object; // bukarest
//				show(subject,true);
				continue;
			}

			if (!value) {//<address> <city> ...
//				parents.push(subject);
				parent=subject;
				object = add(field);
				addStatement(subject, Member, object,DONT_CHECK_DUPLICATES);
				if(!contains(line,"><"))// else empty!
				subject = object;
				addAttributes(subject, line);
				continue;
			}
			if (hasAttribute(line)) {
				predicate = add(field); // <zip color='green'>12345</zip>
				addAttributes(predicate, line);
			} else {
				predicate = getThe(field, NO_TYPE, DONT_DISSECT);
			}
			object = getThe(value, NO_TYPE, DONT_DISSECT);
			addStatement(subject, predicate, object,DONT_CHECK_DUPLICATES);
			//			fields.insert(predicate,object);//
			continue;
		}
		if (startsWith(line, "<") && !contains(line, "</")) {
			parent=subject;
//			parents.push(subject); // <address> <city> ...
			field = extractTagName(line);
			subject = add(field); // can be replaced by name!
			addStatement(parent, Member, subject,DONT_CHECK_DUPLICATES); // address.city
			addAttributes(subject, line);
			continue;
		}
	}
	fclose(infile); /* Close the file */
	p("import xml ok ... items imported:");
	pi(linecount);
}

void importCsv(const char* facts_file, Node* type, const char* separator, const char* ignoredFields, const char* includedFields, int nameRowNr, const char* nameRow) {
	p("import csv start");
	char line[1000];
	char** values = (char**) malloc(sizeof (char*) *100);
	memset(values, 0, 100);
	//    vector<char*> values;

	Node* subject=0;
	Node* predicate=0;
	Node* object=0;
	vector<Node*> predicates = *new vector<Node*>();
	vector<char*> ignoreFields = splitString(ignoredFields, ",");
	vector<char*>& includeFields = splitString(includedFields, ",");
	vector<char*>& fields = *new vector<char*>();
	int linecount = 0;
	FILE *infile;
	printf("Opening File %s\n", (facts_file));
	if ((infile = fopen((facts_file), "r")) == NULL)facts_file = (basepath + facts_file).c_str();
	if ((infile = fopen((facts_file), "r")) == NULL) {
		perror("Error opening file");
		exit(1);
	}
	char* objectName = (char*) malloc(100);
	int fieldCount = 0;
	char* columnTitles;
	while (fgets(line, sizeof (line), infile) != NULL) {
		if (!line)break;
		if (linecount == 0) {
			columnTitles = line;
			if (fields.size() == 0)
				nameRowNr = getFields(line, fields, separator, nameRowNr, nameRow);
			fieldCount = fields.size();
			for (int i = 0; i < fieldCount; i++) {
				char* field = fields.at(i);
				predicates.push_back(getThe(field));
			}
			++linecount;
			continue;
		}
		if (++linecount % 100 == 0)printf("%d\n", linecount);

		fixNewline(line);
		//        values.erase(values.begin(),values.end());
		//        ps(line);

		int size = splitStringC(line, values, separator);
		if (fieldCount != size) {
			printf("Warning: fieldCount!=columns in line %d   (%d!=%d)\n%s\n", linecount - 1, fieldCount, size, line);
			//            ps(columnTitles); // only 1st word:
			//            ps(line);// split! :{
			continue;
		}
		bool dissect = type && !eq(type->name, "city"); // city special: too many!
		// todo more generally : don't dissect special names ...

		subject = getThe(values[nameRowNr], null, dissect);
		if (type && subject->kind != type->id) {
			//			p("Found one with different type");
			Node* candidate = subject;
			subject = getThe(values[nameRowNr], type, dissect); // todo : match more or less strictly? EG Hasloh
			addStatement4(type->context, candidate->id, Synonym->id, subject->id, true);
			//			addStatement4(type->context,candidate->id,Unknown->id,subject->id,false);
		}
		//		if(eq(subject->name,"Hasloh"))
		//			linecount++;
		for (int i = 0; i < size; i++) {
			if (i == nameRowNr)continue;
			predicate = predicates[i];
			if (predicate == null)continue;
			if (contains(ignoreFields, predicate->name))continue;
			if (includedFields != null && !contains(includeFields, predicate->name))continue;
			char* vali = values[i];
			if (!vali || strlen(vali) == 0)continue; //HOW *vali<100??
			object = getThe(vali);
			Statement *s = addStatement(subject, predicate, object, false);
			//            showStatement(s);
		}

		if (!subject || !predicate || !object || subject->id > maxNodes || object->id > maxNodes) {
			printf("Quitting import : id > maxNodes\n");
			break;
		}
	}
	fclose(infile); /* Close the file */
	p("import csv ok ... lines imported:");
	pi(linecount);
}

void importList(const char* facts_file, const char* type) {
	p("import list start");
	char line[1000];
	Node* subject = getClass(type);
	Node* object;
	int linecount = 0;
	FILE *infile;
	printf("Opening File %s\n", (facts_file));
	if ((infile = fopen((facts_file), "r")) == NULL) {
		perror("Error opening file");

		exit(1);
	}
	while (fgets(line, sizeof (line), infile) != NULL) {
		if (++linecount % 10000 == 0)printf("%d\n", linecount);
		char* objectName = (char*) malloc(100);
		object = getThe(line);
		addStatement(subject, Instance, object, false);
		if (!subject || !object || subject->id > maxNodes || object->id > maxNodes) {
			printf("Quitting import : id > maxNodes\n");
			break;
		}
	}
	fclose(infile); /* Close the file */
	p("import list ok");
}

bool importFacts(const char* facts_file, const char* predicateName = "population") {
	p("import facts start");
	Node* subject;
	Node* predicate;
	Node* object;
	char line[1000];
	//    char* predicateName=(char*) malloc(100);
	predicate = getClass(predicateName);
	int linecount = 0;
	FILE *infile;
	printf("Opening File %s\n", facts_file);
	if ((infile = fopen(facts_file, "r")) == NULL) {
		perror("Error opening file");
		return false;
		//        exit(1);
	}
	char* objectName = (char*) malloc(100);
	char* subjectName = (char*) malloc(100);
	while (fgets(line, sizeof (line), infile) != NULL) {
		/* Get each line from the infile */
		if (++linecount % 10000 == 0)printf("%d\n", linecount);
		int contextId;
		//	char* predicateName=(char*) malloc(100);
		int subjectId;
		int predicateId;
		int objectId;
		int id; // ignore now!!
		int certainty;

		if (!eq(predicateName, "population"))
			sscanf(line, "%s\t%s", subjectName, objectName); // no contextId? cause this db assumes GLOBAL id!
		else
			sscanf(line, "%d\t%s\t%s\t%d", &id, subjectName, objectName, &certainty); // no contextId? cause this db assumes GLOBAL id!
		// printf(line);
		//	 printf("%d\t%d\t%d\n",subjectId,predicateId,objectId);

		// important words first!!
		//if(contains(subjectName,"_") || contains(subjectName,"-") || contains(subjectName,":") || contains(subjectName,"#"))
		//    continue;
		if (contains(subjectName, "Begriffskl") || contains(subjectName, "Abkürzung") || contains(subjectName, ":") || contains(subjectName, "#"))
			continue;
		if (contains(objectName, "jpg") || contains(objectName, "gif") || contains(objectName, "svg") || contains(objectName, "#") || contains(objectName, ":"))
			continue;

		//     subject=getThe(subjectName);
		subject = getAbstract(subjectName); //

		//    show(subject);
		//    int objectValue=atoi(objectName);
		//    if(eq(predicateName,"population")&&objectValue<1000)continue;

		//if(contains(objectName,",") || contains(objectName,"("))
		//    object=getThe(objectName);
		//    else
		object = getAbstract(objectName);
		//dissectWord(abstract);

		Statement* s;

		if (contains(objectName, subjectName, true))
			s = addStatement(subject, Member, object, false); // todo: id
		else
			s = addStatement(subject, predicate, object, false); // todo: id

		if (!subject || !object || subject->id > maxNodes || object->id > maxNodes) {
			printf("Quitting import : id > maxNodes\n");
			break;
		}

		showStatement(s);
	}
	fclose(infile); /* Close the file */
	p("import facts ok");
	return true;
}

int collectAbstracts() {
	ps("collecting abstracts");
	initRelations();
	abstracts = (Ahash*) (&root_memory[abstractsOffset]);
	extrahash = (Ahash*) (&root_memory[abstractsOffset + abstractHashSize]);
	memset(abstracts, 0, abstractHashSize);
	memset(extrahash, 0, abstractHashSize);
	Context* c = currentContext();
	int max = c->nodeCount; // maxNodes;
	int count = 0;
	// collect Abstracts
	for (int i = 0; i < max; i++) {
		Node* n = &c->nodes[i];
		if (i > 1000 && !checkNode(n))break;
		if (n == null || n->name == null || n->id == 0 || n->context == 0)
			continue;
		if (n->kind == Abstract->id) {
			//			if(eq(n->name,"city"))
			//				max--;
			insertAbstractHash(n);
			count++;
		}
	}
	return count;
}

void fixNodeNames(Context* context, char* oldnodeNames) {

#ifdef inlineName
	printf("inlineNames!");
	return;
#endif
	int max = context->nodeCount; // maxNodes;
	long newOffset= context->nodeNames- oldnodeNames;
	for (int i = 0; i < max; i++) {
		Node* n = &context->nodes[i];
//		show(n,true);
		if (n == null || n->name == null || n->id == 0 || n->context == 0)
			continue;
#ifndef inlineName
		n->name = n->name + newOffset;
#endif

	}
}

void fixNodes(Context* context, Node* oldNodes) {
	int max = context->statementCount; // maxStatements;
	for (int i = 0; i < max; i++) {
		Statement* n = &context->statements[i];

		if(!checkStatement(n)){
			showStatement(n);continue;
		}
		//		if(n==null || n->id==0 || n->context==0)
		//			continue;
		//			n->Subject=n->Subject-oldNodes+context->nodes;
		//			n->Predicate=n->Predicate-oldNodes+context->nodes;
		//			n->Object=n->Object-oldNodes+context->nodes;
		n->Subject = &context->nodes[n->subject];
		n->Predicate = &context->nodes[n->predicate];
		n->Object = &context->nodes[n->object];
		//		    n->name=n->name-oldnodeNames+c->nodeNames;
	}
}

void load(bool force) {

	clock_t start = clock();
	double diff;
	//  diff = ( std::clock() - start ) / (double)CLOCKS_PER_SEC;

	Context* c = currentContext();
	//    showContext(c->id);
	Node* oldNodes = c->nodes;
	char* oldnodeNames = c->nodeNames;
	oldnodeNames = initContext(c);

	//  #include <sys/stat.h>
	//  struct stat stFileInfo;
	//  int intStat = stat((path+ "contexts.bin").data(),&stFileInfo);
	//  if(intStat != 0) { p("file not found");}
	//
	if (!force && root_memory) {//&&root_memory[1000]!=0){// first id==0
		//        pi(root_memory[0]);
		ps("loaded from shared memory");
		if (virgin_memory || !hasWord("Member"))
			collectAbstracts(); //or load abstracts->bin
		showContext(wordnet);
		return;
	}

	ps("Loading graph from files!");

	FILE *fp;
	printf("Opening File %s\n", (path + "contexts.bin").data());
	if ((fp = fopen((path + "contexts.bin").data(), "rb")) == NULL) {
		perror("Error opening file");
		p("starting with fresh context!");
		//        exit(1);
	} else {
		fread(contexts, sizeof (Context), maxContexts, fp);
		fclose(fp);
	}
	//    fp=fopen("wordnet.bin", "rb");
	//    fread(c, sizeof(Context), 1, fp);
	//    fclose (fp);

	fp = fopen((path + "names.bin").data(), "rb");
	fread(c->nodeNames, sizeof (char), c->currentNameSlot + 100, fp);
	fclose(fp);

	fp = fopen((path + "statements.bin").data(), "rb");
	fread(c->statements, sizeof (Statement), c->statementCount, fp); //c->statementCount maxStatements
	fclose(fp);

	fp = fopen((path + "nodes.bin").data(), "rb");
	fread(c->nodes, sizeof (Node), c->nodeCount, fp);

	//
	//    fp = fopen((path+"abstracts->bin").data(), "rb");
	//    fread(abstracts, sizeof (Node), c->nodeCount, fp);

	//    showContext(c);

#ifndef inlineName
	if (oldnodeNames != c->nodeNames) {
		p("Fixing nodeNames");
		fixNodeNames(c, oldnodeNames);
		//        init();// to get back relations CAREFUL!
	}
#endif

	if (oldNodes != c->nodes) {
		p("Fixing nodes");
		fixNodes(c, oldNodes);
	}

	collectAbstracts(); //or load abstracts->bin

	//    initContext(c);//
	//
	//
	//    fp=fopen("test.bin", "rb");
	//    if(fp)fread(test, sizeof(char), 100, fp);
	//   fclose (fp);

	//  cout<<"nanoseconds "<< diff <<'\n';
}

void fixPointers() {
	Context* context = (Context*) root_memory;
	showContext(context);
	fixPointers(context);
	showContext(context);
	context = currentContext();// &contexts[wordnet]; // todo: all
	showContext(context);
	fixPointers(context);
	showContext(context);
	collectAbstracts();
}

void fixPointers(Context* context) {
	p("ADJUSTING SHARED MEMORY");
	Node* oldNodes = context->nodes;
	char* oldNames = context->nodeNames;
	initContext(context);
	check((char*) context->nodes == root_memory + contextOffset);
	//	context->nodes=root_memory
	fixNodes(context, oldNodes);
	fixNodeNames(context, oldNames);
}


#define a(word) getThe(#word)
#define all(word) getThe(#word)

void importNames() {
	importList((basepath + "FrauenVornamen.txt").data(), "female_firstname");
	importList((basepath + "MaennerVornamen.txt").data(), "male_firstname");
	addStatement(all(firstname), are, a(name));
	addStatement(all(male_firstname), a(gender), a(male));
	addStatement(all(male_firstname), Owner, a(male));
	addStatement(all(female_firstname), a(gender), a(female));
	addStatement(all(female_firstname), Owner, a(female));
}

void importWordnet() {
	importNodes(); // FIRST! Hardlinked ids overwrite everything!!
	importStatements();
}

void importGeoDB() {
	importCsv("/Users/me/data/base/geo/geonames/cities1000.txt",\
			getThe("city"), "\t", "alternatenames,modificationdate,geonameid",\
		"latitude,longitude,population,elevation,countrycode", 2, "asciiname");
}

// IMPORTANT: needs manual collectAbstracts() afterwards (for speed reasons??)

void importAll() {
	//	importFacts()
	importCsv("adressen.txt");
	importNames();
	importWordnet();
	importGeoDB();
	//    importImages();
}

void importWikipedia() {

}

void import(const char* filename, const char* path0) {
	basepath = path0;

	clock_t start;
	double diff;
	//  start = clock();
	//  diff = ( std::clock() - start ) / (double)CLOCKS_PER_SEC;

	if (eq(filename, "all")) {
		importAll();
	} else if (eq(filename, "csv")) {
		importCsv(path0);
	} else if (eq(filename, "wordnet")) {
		importWordnet();
	} else if (eq(filename, "names")) {
		importNames();
	} else if (eq(filename, "images")) {
		importImages();
	} else if (eq(filename, "wiki")) {
		importWikipedia();
	} else if (eq(filename, "topic")) {
		importWikipedia();
	} else if (eq(filename, "yago")) {
		importFacts(filename, filename);
	} else if (contains(filename, "txt")) {
		importCsv(filename);
	} else if (contains(filename, "csv")) {
		importCsv(filename);
	} else if (contains(filename, "tsv")) {
		importCsv(filename);
	} else if (contains(filename, "xml")) {
		importXml(filename);
	} else if (!importFacts(filename, filename))
		importAll();


	//  cout<<"nanoseconds "<< diff <<'\n';

	// importSqlite(filename);
	//    importNodes();
	//    importStatements();
}
/*root@h1642655:~/netbase# l facts/
actedIn           during              hasChild           hasISBN                hasRevenue       interestedIn    isSubstanceOf    subClassOf
bornIn            establishedOnDate   hasCurrency        hasLabor               hasSuccessor     inTimeZone      livesIn          subPropertyOf
bornOnDate        exports             hasDuration        hasMotto               hasTLD           inUnit          locatedIn        type
created           familyNameOf        hasEconomicGrowth  hasNominalGDP          hasUnemployment  isAffiliatedTo  madeCoverFor     until
createdOnDate     foundIn             hasExpenses        hasNumberOfPeople      hasUTCOffset     isCalled        means            using
dealsWith         givenNameOf         hasExport          hasOfficialLanguage    hasValue         isCitizenOf     musicalRole      worksAt
describes         graduatedFrom       hasGDPPPP          hasPages               hasWaterPart     isLeaderOf      originatesFrom   writtenInYear
diedIn            happenedIn          hasGini            hasPopulation          hasWebsite       isMarriedTo     participatedIn   wrote
diedOnDate        hasAcademicAdvisor  hasHDI             hasPopulationDensity   hasWeight        isMemberOf      politicianOf
directed          hasArea             hasHeight          hasPoverty             hasWonPrize      isNativeNameOf  produced
discovered        hasBudget           hasImdb            hasPredecessor         imports          isNumber        publishedOnDate
discoveredOnDate  hasCallingCode      hasImport          hasProduct             influences       isOfGenre       range
domain            hasCapital          hasInflation       hasProductionLanguage  inLanguage       isPartOf        since
 */
